
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage[numbers]{natbib}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\title{Einstiegspunkte für Design und Codierung \\bei einer Wartungsaufgabe oder Fehlermeldung}

\titlerunning{Einstiegspunkte für Design und Codierung}

\author{Felix Fröhlich \and Thorben Wiese}

\authorrunning{Felix Fröhlich \and Thorben Wiese}

\institute{Universität Hamburg \\
Fakultät für Mathematik,\\
Informatik und Naturwissenschaften \\
Department Informatik}

\maketitle

\begin{abstract}
Bei der Entwicklung und Wartung von Software spielt die Identifikation des Nutzen von Programmabschnitten eine große Rolle, um entsprechende Funktionen einer Software zu ändern oder zu reparieren. Diese \textit{Feature Locations} stellen einen Einstigespunkt in den Quelltext für Design- oder Code-Änderungen dar und können mithilfe verschiedener Technologien ermittelt werden. In dieser Seminararbeit stellen wir unterschiedliche Technologien und deren Verfahren vor und geben einen Überblick über geeignete Nutzungsfelder.
\end{abstract}

\section{Einleitung}

%TODO Am Ende schreiben?
Die erste Quelle \cite{survey}.\\
...\\
Ziel dieser Seminararbeit ist es, die verschiedenen Analyseverfahren und Technologien zur Erkennung von Features im Code zu beschreiben und zu vergleichen.

\section{Begriffe}

Für die Vorstellung der Analyseverfahren sollen zunächst einige Begriffe definiert und erklärt werden.

\subsection*{Feature}
Ein Feature ist ein Software Artefakt, das eine spezifische Funktionalität implementiert \cite{feature}. Diese Funktionalität wird in natürlicher Sprache beschrieben und wird von einem Programmabschnitt wiedergespiegelt. Ein Feature besteht üblicherweise aus einen Namen, einer Bedeutung (Intension) und einer Erweiterung (Extension) \cite{rajlich-chen}.

\subsection*{Feature Location}
Der Prozess der Feature Location beschreibt die Identifikation der Beziehung zwischen Features und deren Implementierung. Dabei liegt die Beschreibung des Features in natürlicher Sprache vor, die dann einem entsprechenden Codeabschnitt zugeordnet werden soll (Mapping) \cite{survey}.

\section{Analyseverfahren}

In diesem Abschnitt sollen verschiedene Analyseverfahren vorgestellt werden, die das Finden von Feature Locations ermöglichen.

%\subsection*{Formal Concept Analysis (FCA)}

%Thorben
\subsection*{Program Dependence Analysis (PDA)}

Die Analyse eines Programms auf interne Abhängigkeiten wird Abhängigkeitsanalyse (engl. Dependence Analysis) genannt. Sie umfasst in der Regel Kontrollflussabhängigkeiten und Datenabhängigkeiten innerhalb eines Programmes. Diese werden zur Übersetzungszeit mithilfe des Compilers festgestellt. Ziel dieser Analyse ist es zum Beispiel, zu überprüfen, ob ein Programm parallelisiert ausgeführt werden kann. Für die Analyse von Features im Quelltext ist diese Methode hilfreich, da durch sowohl Kontrollflussabhängigkeiten, als auch Datenabhängigkeiten auch semantische Verknüpfungen verschiedener Abschnitte des Codes hergestellt werden können \cite{PDA}.

%Felix
\subsection*{Trace Analysis}

%Thorben
\subsection*{Latent Semantic Indexing (LSI)}

Das Verfahren des Latent Semantix Indexing wird zum Indexieren von Abschnitten und Mustern eines Textes mithilfe von Singulärwertzerlegung der Ausdruck-Dokument-Matrix verwendet. Die Ausdruck-Dokument-Matrix (engl. Document-Term-Matrix) stellt die Frequenz der Ausdrücke innerhalb eines Dokumentes oder Textes dar. Die Singulärwertzerlegung dieser Matrix ist die Darstellung der ursprünglichen Matrix als Produkt dreier spezieller Matrizen, deren Singulärwerte auf bestimmte Eigenschaften der Matrix schließen lassen. LSI geht davon aus, dass Wörter, die in einem bestimmten Kontext verwendet werden, eine ähnliche Bedeutung haben. Dieses Verfahren lässt sich von natürlicher Sprache auch auf Quellcode übertragen, insbesondere auf Kommentare innerhalb des Codes \cite{LSI}.

%\subsection*{Term Frequency - Inverse Document Frequency Matrix}

%\subsection*{Hyper-link Induced Topic Search (HITS)}

% technologies: PDA, LSI, Trace Analysis

%Thorben
\subsection{Statische Feature Location Technologien}

In diesem Abschnitt sollen statische Technologien zum Finden von Features im Quelltext vorgestellt werden.

\subsection*{Statische Analyse}

Die Analyse von Quelltext zu einem Zeitpunkt, bei dem das Programm kompiliert wird und noch nicht ausgeführt wurde, wird statische Analyse genannt. Hierbei werden mithilfe von zum Beispeil Datenfluss-Analyse und Kontrollgraphen alle Abhängigkeiten und Funktionsaufrufe innerhalb des Codes analysiert und es können unter anderem Fehler wie zum Beispiel Race-Conditions oder Buffer-Overflows identifiziert werden. Dieser Prozess wird häufig von automatisierten Tools durchgeführt \cite{static}.

\subsection*{Technologie Beispiele}

Robillard et al. [35]\\
Shao et al. [40]

%Felix
\subsection{Dynamische Feature Location Technologien}


\subsection*{Dynamische Analyse}
Allgemein dynamische Analyse
Hauptmerkmal der dynamischen Analyse ist ihre Durchführung während der Laufzeit eines Programms. Das bedeutet die Ausführung des Programms ist zwingend erforderlich und während der Laufzeit werden Informationen gesammelt um wahrscheinliche Features zu lokalisieren. Dabei können ausschließlich funktionale Features gefunden werden, da die dynamische Analysetechnik an eine eingeschränkte Sicht auf das Programm gebunden ist, die der Sicht des Anwenders entspricht. \\Ähnlich wie bei der statischen Analyse unterscheidet man zwischen Plain-Output-Techniken, bei denen mögliche, zu einem Feature gehörende, Quellcodebestandteile ungeordnet zurückgeliefert werden und Guided-Output-Techniken, bei denen die ausgegebenen Komponenten durch zusätzliche Informationen in Zusammenhang gebracht werden und ihr Kontext genauer erläutert wird. 

\subsection*{Technologie Beispiel - Plain Output}
In ihrer wissenschaftlichen Arbeit Location Program Features by using  Execution Slices benutzen die Autoren ein dynamisches Plain-Output-Verfahren zur Analyse von Code. Sie verwenden dabei Execution Slicing im Gegensatz zum Static Slicing. Beide 
Techniken werden unter Program Slicing zusammengefasst.\\
Slices können verschiedene Bestandteile eines Programms sein wie z.B. Code-Blöcke oder Kontrollstrukturen. Auch sogenannte c- und p-uses können Bestandteile einer Slice sein.\\ C- und p - uses beschreiben beide den Pfad einer Variable im Programmfluss ohne, dass die Variable verändert wird, wobei c-uses Variablen zur Berechnung dienen und p-uses-Variablen für eine Prädikat oder eine Entscheidung.\\
Faktoren, wie Heuristiken, Test Cases, Detailgenauigkeit und Tool-Unterstützung sind ausschlaggebend für den Erfolg einer dynamischen Analysetechnik. \\
Besonderem Augenmerk gilt dabei der Menge der Tests, die dem Programm als Eingabe übergeben werden. Die Autoren heben hervor wie wichtig die richtigen Testmengen sind und unterscheiden zwischen aufrufenden (invoking) tests, die sich explizit auf ein bestimmtes Feature und seine Funktionalität fokussieren und ausschließenden Tests, die alle anderen Test-Sets enthalten, die nicht auf das Feature fokussiert sind worunter aber auch die zu finden sind, welche das gesuchte Feature und weitere Komponenten umfassen d.h. Tests bei denen das Feature nur ein Bestandteil von vielen ist.
\\ Eine für die Autoren erfolgreiche Herangehensweise war zuerst die Menge aller aufrufenden Tests bilden von denen einige auch die Funktionalität anderer Features testen. Dann bildet man die Menge aller ausschließenden Tests, die jedoch auch Codeabschnitte enthalten können, die mit dem Feature interagieren. Die Differenz der beiden ist die Teilmenge, die zur Lokalisierung des gesuchten Features genutzt wird. \\
Die aufrufenden Tests sollen sich dabei sehr stark unterscheiden, während die ausschließenden Tests sehr ähnlich sein sollen. \\
Ohne Tool-Unterstützung wäre es sehr Aufwending Slices, die wie erwähnt aus Code-Blöcken, Kontrollstrukturen, c- oder p-uses bestehen, für jeden einzelnen Testfall zu sammeln. Die Autoren verwendeten deshalb xVue. Dieses Programm zählt für jeden Test wie oft die angegebenen execution slices verwendet wurden und ordnet dadurch jedem Test die dazugehörigen Slices zu. Weiterhin teilt es die Tests, sofern dies noch nicht geschehen ist, in aufrufende und ausschließenden Tests. \\

Wong et al. [49]\\
Liu et al. [25] (SITIR)

\subsection{Textuelle Feature Location Technologien}
Allgemein textuelle Analyse, Tools beschreiben, Beispiele

\section{Vergleich}
Die statische Analyse untersucht den Quelltext vor der Ausführung und hat somit Zugriff auf sämtliche Bestandteile des Programms. Dadurch können Features
jeglicher Art gefunden werden, seien sie nun funktional oder nicht nicht-funktional. Eine mehrmalige Wiederholung der statischen Analyse eines
Programms ist idempotent und liefert immer wieder das gleiche Ergebnis. \\
Während also die statische Analyse einen größeren Feature-Raum untersuchen kann bezüglich ihrer Quantität und Diversität, ist die dynamische
Analyse an das Programm zu Ausführung gebunden und damit auch an spezifische Eingabeparameter, die den Verlauf bestimmen. Somit kann das Auffinden
von Features stark eingeschränkt sein und selbst ein gefundenes Feature lässt sich nur schwer generalisieren [20]. \\
Betrachtet man Verfahren zu statischen und dynamischen Analyse als binären Klassifikator, dann liefern statische Verfahren mit größerer 
Wahrscheinlichkeit eine Menge false-positives. Diese resultieren nicht nur aus den quantitativ höheren Featureraum sondern auch aus
der Feststellung, dass die Wahrscheinlichkeit eines gefundenen Features nie genau bestimmt sondern nur approximiert werden kann. Statische
Verfahren finden also mit größerer Wahrscheinlichkeit Features, die eigentlich gar keine sind, die sogenannte Überapproximiation. \\
Dynamische Verfahren dagegen, auf einen kleineren Feature-Raum beschränkt, liefern dagegen mehr false-negatives. Existierende Features
werden in diesem Fall nicht als solche erkannt und falsch klassifiziert was man als Unterapproximation bezeichnet.

\section{Fazit}
Für welchen Zweck welches Analyseverfahren und welche Technologie
Ergebnis wahrscheinlich: Alles gar nicht so schlecht, je nach Bedarf muss eine Technologie ausgewählt werden oder eventuell mit einer anderen kombiniert werden.



%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%												LITERATURVERZEICHNIS 												       %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\phantomsection % benötigt für korrekte pdf-darstellung
\addcontentsline{toc}{section}{Literaturverzeichnis}
\bibliographystyle{vancouver} % Din 1505 nach Lorenzen (Das konkrete Aussehen des Litverzeichnisses ist im header festgelegt)
\setcitestyle{square}
\bibliography{literatur}  % Pfad zur *.bib-Datei (Dateiendung wird weggelassen)

\end{document}
